{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import Model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('trmodel', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gidiyor', 0.592621922492981),\n",
       " ('gidiyorum', 0.5690363645553589),\n",
       " ('gidelim', 0.5637925863265991),\n",
       " ('geldim', 0.5413458943367004),\n",
       " ('bakıyor', 0.5373592376708984),\n",
       " ('gittim', 0.5343413949012756),\n",
       " ('gideceğim', 0.5251941680908203),\n",
       " ('geldik', 0.505060076713562),\n",
       " ('geliyoruz', 0.5047824382781982),\n",
       " ('gider', 0.504159688949585)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "word_vectors.most_similar(positive=[\"geliyor\",\"gitmek\"],negative=[\"gelmek\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ve</th>\n",
       "      <td>0.296941</td>\n",
       "      <td>-0.269876</td>\n",
       "      <td>-1.180450</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>-0.286231</td>\n",
       "      <td>1.286693</td>\n",
       "      <td>-0.886565</td>\n",
       "      <td>0.358893</td>\n",
       "      <td>-0.773943</td>\n",
       "      <td>1.468419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650401</td>\n",
       "      <td>-0.742386</td>\n",
       "      <td>0.142566</td>\n",
       "      <td>0.559789</td>\n",
       "      <td>-0.762779</td>\n",
       "      <td>-0.337023</td>\n",
       "      <td>-0.248934</td>\n",
       "      <td>1.191360</td>\n",
       "      <td>0.110982</td>\n",
       "      <td>0.160032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kategori</th>\n",
       "      <td>-1.299320</td>\n",
       "      <td>0.990288</td>\n",
       "      <td>0.307114</td>\n",
       "      <td>-1.498320</td>\n",
       "      <td>1.659920</td>\n",
       "      <td>0.028483</td>\n",
       "      <td>-0.201647</td>\n",
       "      <td>2.227363</td>\n",
       "      <td>1.416063</td>\n",
       "      <td>0.608082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449724</td>\n",
       "      <td>1.288062</td>\n",
       "      <td>-1.129284</td>\n",
       "      <td>-3.064066</td>\n",
       "      <td>-0.667573</td>\n",
       "      <td>-1.473625</td>\n",
       "      <td>-0.382927</td>\n",
       "      <td>-1.564002</td>\n",
       "      <td>1.952312</td>\n",
       "      <td>2.889835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bir</th>\n",
       "      <td>0.510168</td>\n",
       "      <td>-0.418357</td>\n",
       "      <td>-1.772496</td>\n",
       "      <td>0.227772</td>\n",
       "      <td>-0.475176</td>\n",
       "      <td>0.432043</td>\n",
       "      <td>-3.295489</td>\n",
       "      <td>0.403906</td>\n",
       "      <td>-0.536450</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>...</td>\n",
       "      <td>1.942218</td>\n",
       "      <td>0.180768</td>\n",
       "      <td>0.778027</td>\n",
       "      <td>1.420458</td>\n",
       "      <td>-0.523086</td>\n",
       "      <td>0.714488</td>\n",
       "      <td>0.296618</td>\n",
       "      <td>1.671680</td>\n",
       "      <td>0.358448</td>\n",
       "      <td>-0.116159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.203403</td>\n",
       "      <td>0.377098</td>\n",
       "      <td>-2.617118</td>\n",
       "      <td>-1.166068</td>\n",
       "      <td>0.350960</td>\n",
       "      <td>0.072562</td>\n",
       "      <td>-1.258488</td>\n",
       "      <td>0.911878</td>\n",
       "      <td>-0.665523</td>\n",
       "      <td>1.468329</td>\n",
       "      <td>...</td>\n",
       "      <td>2.019327</td>\n",
       "      <td>0.436097</td>\n",
       "      <td>-1.341626</td>\n",
       "      <td>2.594386</td>\n",
       "      <td>-1.235236</td>\n",
       "      <td>0.492994</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.438153</td>\n",
       "      <td>-1.542385</td>\n",
       "      <td>0.250436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0.477729</td>\n",
       "      <td>0.802959</td>\n",
       "      <td>-2.586396</td>\n",
       "      <td>-1.524061</td>\n",
       "      <td>0.580028</td>\n",
       "      <td>-0.531550</td>\n",
       "      <td>-1.301819</td>\n",
       "      <td>0.316712</td>\n",
       "      <td>-0.331988</td>\n",
       "      <td>1.520155</td>\n",
       "      <td>...</td>\n",
       "      <td>1.572341</td>\n",
       "      <td>1.336991</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>0.022804</td>\n",
       "      <td>0.143684</td>\n",
       "      <td>-0.769812</td>\n",
       "      <td>1.441397</td>\n",
       "      <td>-1.471803</td>\n",
       "      <td>1.120255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zırhlardan</th>\n",
       "      <td>-0.037423</td>\n",
       "      <td>-0.064962</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>-0.027573</td>\n",
       "      <td>0.048518</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.002876</td>\n",
       "      <td>0.051829</td>\n",
       "      <td>-0.019547</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033733</td>\n",
       "      <td>-0.029264</td>\n",
       "      <td>0.033644</td>\n",
       "      <td>0.049926</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>-0.012362</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.031783</td>\n",
       "      <td>-0.019363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanklardı</th>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.068778</td>\n",
       "      <td>-0.043494</td>\n",
       "      <td>0.077601</td>\n",
       "      <td>0.027750</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>-0.033677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016961</td>\n",
       "      <td>-0.052620</td>\n",
       "      <td>-0.019530</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>0.067636</td>\n",
       "      <td>-0.028386</td>\n",
       "      <td>0.064698</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>-0.044515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bēkon</th>\n",
       "      <td>-0.043483</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>-0.005685</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.012918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045089</td>\n",
       "      <td>-0.002602</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.005691</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>-0.002974</td>\n",
       "      <td>-0.049444</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>-0.027719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coosemans</th>\n",
       "      <td>-0.047166</td>\n",
       "      <td>-0.025059</td>\n",
       "      <td>0.031535</td>\n",
       "      <td>0.039888</td>\n",
       "      <td>-0.013063</td>\n",
       "      <td>0.030065</td>\n",
       "      <td>0.038028</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>0.089070</td>\n",
       "      <td>-0.033487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026820</td>\n",
       "      <td>-0.039541</td>\n",
       "      <td>-0.033091</td>\n",
       "      <td>0.044215</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>-0.001221</td>\n",
       "      <td>-0.031015</td>\n",
       "      <td>-0.053000</td>\n",
       "      <td>-0.043469</td>\n",
       "      <td>-0.069313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halett</th>\n",
       "      <td>-0.011931</td>\n",
       "      <td>-0.023241</td>\n",
       "      <td>-0.014656</td>\n",
       "      <td>0.042559</td>\n",
       "      <td>0.029065</td>\n",
       "      <td>0.046308</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>-0.016630</td>\n",
       "      <td>0.080124</td>\n",
       "      <td>-0.013916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039847</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>-0.060231</td>\n",
       "      <td>0.023045</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>-0.013940</td>\n",
       "      <td>0.040766</td>\n",
       "      <td>-0.066751</td>\n",
       "      <td>-0.057071</td>\n",
       "      <td>-0.042234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412457 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5    \\\n",
       "ve          0.296941 -0.269876 -1.180450  0.190302 -0.286231  1.286693   \n",
       "kategori   -1.299320  0.990288  0.307114 -1.498320  1.659920  0.028483   \n",
       "bir         0.510168 -0.418357 -1.772496  0.227772 -0.475176  0.432043   \n",
       "da          0.203403  0.377098 -2.617118 -1.166068  0.350960  0.072562   \n",
       "de          0.477729  0.802959 -2.586396 -1.524061  0.580028 -0.531550   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "zırhlardan -0.037423 -0.064962  0.003443 -0.006851 -0.027573  0.048518   \n",
       "tanklardı  -0.008692  0.008498  0.018832  0.068778 -0.043494  0.077601   \n",
       "bēkon      -0.043483  0.022171  0.011649  0.008683  0.004279 -0.005566   \n",
       "coosemans  -0.047166 -0.025059  0.031535  0.039888 -0.013063  0.030065   \n",
       "halett     -0.011931 -0.023241 -0.014656  0.042559  0.029065  0.046308   \n",
       "\n",
       "                 6         7         8         9    ...       390       391  \\\n",
       "ve         -0.886565  0.358893 -0.773943  1.468419  ... -0.650401 -0.742386   \n",
       "kategori   -0.201647  2.227363  1.416063  0.608082  ... -0.449724  1.288062   \n",
       "bir        -3.295489  0.403906 -0.536450  0.002459  ...  1.942218  0.180768   \n",
       "da         -1.258488  0.911878 -0.665523  1.468329  ...  2.019327  0.436097   \n",
       "de         -1.301819  0.316712 -0.331988  1.520155  ...  1.572341  1.336991   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "zırhlardan -0.011570 -0.002876  0.051829 -0.019547  ... -0.033733 -0.029264   \n",
       "tanklardı   0.027750  0.001146  0.020478 -0.033677  ... -0.016961 -0.052620   \n",
       "bēkon       0.018467 -0.005685  0.000652  0.012918  ... -0.045089 -0.002602   \n",
       "coosemans   0.038028  0.020881  0.089070 -0.033487  ... -0.026820 -0.039541   \n",
       "halett     -0.000287 -0.016630  0.080124 -0.013916  ... -0.039847  0.061271   \n",
       "\n",
       "                 392       393       394       395       396       397  \\\n",
       "ve          0.142566  0.559789 -0.762779 -0.337023 -0.248934  1.191360   \n",
       "kategori   -1.129284 -3.064066 -0.667573 -1.473625 -0.382927 -1.564002   \n",
       "bir         0.778027  1.420458 -0.523086  0.714488  0.296618  1.671680   \n",
       "da         -1.341626  2.594386 -1.235236  0.492994  0.000796  0.438153   \n",
       "de          0.004450  0.023776  0.022804  0.143684 -0.769812  1.441397   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "zırhlardan  0.033644  0.049926 -0.015837 -0.012362 -0.005236 -0.059078   \n",
       "tanklardı  -0.019530  0.008937  0.067636 -0.028386  0.064698  0.000106   \n",
       "bēkon       0.001707 -0.005691 -0.009385  0.025023 -0.002974 -0.049444   \n",
       "coosemans  -0.033091  0.044215  0.046135 -0.001221 -0.031015 -0.053000   \n",
       "halett     -0.060231  0.023045  0.009107 -0.013940  0.040766 -0.066751   \n",
       "\n",
       "                 398       399  \n",
       "ve          0.110982  0.160032  \n",
       "kategori    1.952312  2.889835  \n",
       "bir         0.358448 -0.116159  \n",
       "da         -1.542385  0.250436  \n",
       "de         -1.471803  1.120255  \n",
       "...              ...       ...  \n",
       "zırhlardan -0.031783 -0.019363  \n",
       "tanklardı   0.007457 -0.044515  \n",
       "bēkon       0.004175 -0.027719  \n",
       "coosemans  -0.043469 -0.069313  \n",
       "halett     -0.057071 -0.042234  \n",
       "\n",
       "[412457 rows x 400 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_dict = {}\n",
    "for word in word_vectors.index_to_key: \n",
    "    vector_dict[word] = word_vectors.get_vector(word)\n",
    "\n",
    "df = pd.DataFrame(vector_dict).T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoders(Model):\n",
    "\n",
    "    def __init__(self, output_units):\n",
    "        super().__init__()\n",
    "        activ_func = \"LeakyReLU\" \n",
    "        \n",
    "        self.encoder = Sequential(\n",
    "            [\n",
    "                Dense(300, activation=activ_func),\n",
    "                Dense(200, activation=activ_func),\n",
    "                Dense(100, activation=activ_func),\n",
    "                Dense(40, activation=activ_func),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = Sequential(\n",
    "            [\n",
    "                Dense(40, activation=activ_func),\n",
    "                Dense(100, activation=activ_func),\n",
    "                Dense(200, activation=activ_func),\n",
    "                Dense(300, activation=activ_func),\n",
    "                Dense(output_units, activation=\"linear\") \n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = list(vector_dict.values())\n",
    "OUTPUT = list(vector_dict.keys())\n",
    "X = np.asarray(INPUT)\n",
    "y = np.asarray(OUTPUT)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = AutoEncoders(X.shape[1])\n",
    "auto_encoder.compile(\n",
    "    loss='mse',\n",
    "    metrics=['mse'],\n",
    "    optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/4512 [==============================] - 24s 5ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 8.3790e-04 - val_mse: 8.3790e-04\n",
      "Model: \"auto_encoders_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_6 (Sequential)   (None, 40)                204640    \n",
      "                                                                 \n",
      " sequential_7 (Sequential)   (None, 400)               206640    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411280 (1.57 MB)\n",
      "Trainable params: 411280 (1.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_25 (Dense)            (None, 300)               120300    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 40)                4040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204640 (799.38 KB)\n",
      "Trainable params: 204640 (799.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_29 (Dense)            (None, 40)                1640      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 100)               4100      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 200)               20200     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 300)               60300     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 400)               120400    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206640 (807.19 KB)\n",
      "Trainable params: 206640 (807.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 7.4753e-04 - val_mse: 7.4753e-04\n",
      "Epoch 2/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 7.5840e-04 - val_mse: 7.5840e-04\n",
      "Epoch 3/100\n",
      "4512/4512 [==============================] - 30s 7ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 7.3979e-04 - val_mse: 7.3979e-04\n",
      "Epoch 4/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 7.2047e-04 - val_mse: 7.2047e-04\n",
      "Epoch 5/100\n",
      "4512/4512 [==============================] - 27s 6ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 7.2798e-04 - val_mse: 7.2798e-04\n",
      "Epoch 6/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 7.1505e-04 - val_mse: 7.1505e-04\n",
      "Epoch 7/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 7.2978e-04 - val_mse: 7.2978e-04\n",
      "Epoch 8/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 7.2228e-04 - val_mse: 7.2228e-04\n",
      "Epoch 9/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 6.9044e-04 - val_mse: 6.9044e-04\n",
      "Epoch 10/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 7.3458e-04 - val_mse: 7.3458e-04\n",
      "Epoch 11/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 7.0723e-04 - val_mse: 7.0723e-04\n",
      "Epoch 12/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 7.2705e-04 - val_mse: 7.2705e-04\n",
      "Epoch 13/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 7.1722e-04 - val_mse: 7.1722e-04\n",
      "Epoch 14/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 7.0063e-04 - val_mse: 7.0063e-04\n",
      "Epoch 15/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 7.4626e-04 - val_mse: 7.4626e-04\n",
      "Epoch 16/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 7.0767e-04 - val_mse: 7.0767e-04\n",
      "Epoch 17/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 7.5004e-04 - val_mse: 7.5004e-04\n",
      "Epoch 18/100\n",
      "4512/4512 [==============================] - 24s 5ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 7.2276e-04 - val_mse: 7.2276e-04\n",
      "Epoch 19/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 6.9966e-04 - val_mse: 6.9966e-04\n",
      "Epoch 20/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 6.8394e-04 - val_mse: 6.8394e-04\n",
      "Epoch 21/100\n",
      "4512/4512 [==============================] - 20s 5ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 7.1958e-04 - val_mse: 7.1958e-04\n",
      "Epoch 22/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 7.2526e-04 - val_mse: 7.2526e-04\n",
      "Epoch 23/100\n",
      "4512/4512 [==============================] - 20s 5ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 7.0773e-04 - val_mse: 7.0773e-04\n",
      "Epoch 24/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 6.8354e-04 - val_mse: 6.8354e-04\n",
      "Epoch 25/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 7.2876e-04 - val_mse: 7.2876e-04\n",
      "Epoch 26/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 7.0250e-04 - val_mse: 7.0250e-04\n",
      "Epoch 27/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 6.8588e-04 - val_mse: 6.8588e-04\n",
      "Epoch 28/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 7.1367e-04 - val_mse: 7.1367e-04\n",
      "Epoch 29/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 7.2272e-04 - val_mse: 7.2272e-04\n",
      "Epoch 30/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 7.1037e-04 - val_mse: 7.1037e-04\n",
      "Epoch 31/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 7.3109e-04 - val_mse: 7.3109e-04\n",
      "Epoch 32/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 7.4217e-04 - val_mse: 7.4217e-04\n",
      "Epoch 33/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 7.5289e-04 - val_mse: 7.5289e-04\n",
      "Epoch 34/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 7.2199e-04 - val_mse: 7.2199e-04\n",
      "Epoch 35/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 7.1530e-04 - val_mse: 7.1530e-04\n",
      "Epoch 36/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 7.0764e-04 - val_mse: 7.0764e-04\n",
      "Epoch 37/100\n",
      "4512/4512 [==============================] - 26s 6ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 7.0724e-04 - val_mse: 7.0724e-04\n",
      "Epoch 38/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 7.0670e-04 - val_mse: 7.0670e-04\n",
      "Epoch 39/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 7.3113e-04 - val_mse: 7.3113e-04\n",
      "Epoch 40/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 7.3180e-04 - val_mse: 7.3180e-04\n",
      "Epoch 41/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 6.9942e-04 - val_mse: 6.9942e-04\n",
      "Epoch 42/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 6.9162e-04 - val_mse: 6.9162e-04\n",
      "Epoch 43/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 6.9059e-04 - val_mse: 6.9059e-04\n",
      "Epoch 44/100\n",
      "4512/4512 [==============================] - 20s 5ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 7.1992e-04 - val_mse: 7.1992e-04\n",
      "Epoch 45/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 6.8944e-04 - val_mse: 6.8944e-04\n",
      "Epoch 46/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 7.1271e-04 - val_mse: 7.1271e-04\n",
      "Epoch 47/100\n",
      "4512/4512 [==============================] - 20s 5ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 6.8752e-04 - val_mse: 6.8752e-04\n",
      "Epoch 48/100\n",
      "4512/4512 [==============================] - 124s 27ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 6.9368e-04 - val_mse: 6.9368e-04\n",
      "Epoch 49/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 7.1764e-04 - val_mse: 7.1764e-04\n",
      "Epoch 50/100\n",
      "4512/4512 [==============================] - 24s 5ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 6.6862e-04 - val_mse: 6.6862e-04\n",
      "Epoch 51/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 7.1613e-04 - val_mse: 7.1613e-04\n",
      "Epoch 52/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 6.9257e-04 - val_mse: 6.9257e-04\n",
      "Epoch 53/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 7.3057e-04 - val_mse: 7.3057e-04\n",
      "Epoch 54/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 6.9593e-04 - val_mse: 6.9593e-04\n",
      "Epoch 55/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 7.1965e-04 - val_mse: 7.1965e-04\n",
      "Epoch 56/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 7.1869e-04 - val_mse: 7.1869e-04\n",
      "Epoch 57/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 6.8678e-04 - val_mse: 6.8678e-04\n",
      "Epoch 58/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 7.2900e-04 - val_mse: 7.2900e-04\n",
      "Epoch 59/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 7.6273e-04 - val_mse: 7.6273e-04\n",
      "Epoch 60/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 7.0984e-04 - val_mse: 7.0984e-04\n",
      "Epoch 61/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 7.1091e-04 - val_mse: 7.1091e-04\n",
      "Epoch 62/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.3430e-04 - val_mse: 7.3430e-04\n",
      "Epoch 63/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.3165e-04 - val_mse: 7.3165e-04\n",
      "Epoch 64/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.6238e-04 - val_mse: 7.6238e-04\n",
      "Epoch 65/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.0634e-04 - val_mse: 7.0634e-04\n",
      "Epoch 66/100\n",
      "4512/4512 [==============================] - 21s 5ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.4873e-04 - val_mse: 7.4873e-04\n",
      "Epoch 67/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.8164e-04 - val_mse: 7.8164e-04\n",
      "Epoch 68/100\n",
      "4512/4512 [==============================] - 38s 8ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 7.3324e-04 - val_mse: 7.3324e-04\n",
      "Epoch 69/100\n",
      "4512/4512 [==============================] - 41s 9ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 7.1857e-04 - val_mse: 7.1857e-04\n",
      "Epoch 70/100\n",
      "4512/4512 [==============================] - 33s 7ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 6.7948e-04 - val_mse: 6.7948e-04\n",
      "Epoch 71/100\n",
      "4512/4512 [==============================] - 35s 8ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 7.0341e-04 - val_mse: 7.0341e-04\n",
      "Epoch 72/100\n",
      "4512/4512 [==============================] - 26s 6ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 7.6674e-04 - val_mse: 7.6674e-04\n",
      "Epoch 73/100\n",
      "4512/4512 [==============================] - 26s 6ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 6.7338e-04 - val_mse: 6.7338e-04\n",
      "Epoch 74/100\n",
      "4512/4512 [==============================] - 27s 6ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 6.7987e-04 - val_mse: 6.7987e-04\n",
      "Epoch 75/100\n",
      "4512/4512 [==============================] - 24s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 6.8908e-04 - val_mse: 6.8908e-04\n",
      "Epoch 76/100\n",
      "4512/4512 [==============================] - 24s 5ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 7.5218e-04 - val_mse: 7.5218e-04\n",
      "Epoch 77/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 6.9293e-04 - val_mse: 6.9293e-04\n",
      "Epoch 78/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 7.0426e-04 - val_mse: 7.0426e-04\n",
      "Epoch 79/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 7.3136e-04 - val_mse: 7.3136e-04\n",
      "Epoch 80/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 7.3602e-04 - val_mse: 7.3602e-04\n",
      "Epoch 81/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 7.5648e-04 - val_mse: 7.5648e-04\n",
      "Epoch 82/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 7.5719e-04 - val_mse: 7.5719e-04\n",
      "Epoch 83/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.1669e-04 - val_mse: 7.1669e-04\n",
      "Epoch 84/100\n",
      "4512/4512 [==============================] - 24s 5ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.0831e-04 - val_mse: 7.0831e-04\n",
      "Epoch 85/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 6.9475e-04 - val_mse: 6.9475e-04\n",
      "Epoch 86/100\n",
      "4512/4512 [==============================] - 23s 5ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.5046e-04 - val_mse: 7.5046e-04\n",
      "Epoch 87/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.2559e-04 - val_mse: 7.2559e-04\n",
      "Epoch 88/100\n",
      "4512/4512 [==============================] - 22s 5ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.3378e-04 - val_mse: 7.3378e-04\n",
      "Epoch 89/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 6.8268e-04 - val_mse: 6.8268e-04\n",
      "Epoch 90/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.3923e-04 - val_mse: 7.3923e-04\n",
      "Epoch 91/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.2260e-04 - val_mse: 7.2260e-04\n",
      "Epoch 92/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.0859e-04 - val_mse: 7.0859e-04\n",
      "Epoch 93/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 6.9686e-04 - val_mse: 6.9686e-04\n",
      "Epoch 94/100\n",
      "4512/4512 [==============================] - 20s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.1699e-04 - val_mse: 7.1699e-04\n",
      "Epoch 95/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 7.1828e-04 - val_mse: 7.1828e-04\n",
      "Epoch 96/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 6.9850e-04 - val_mse: 6.9850e-04\n",
      "Epoch 97/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 7.0244e-04 - val_mse: 7.0244e-04\n",
      "Epoch 98/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 7.1530e-04 - val_mse: 7.1530e-04\n",
      "Epoch 99/100\n",
      "4512/4512 [==============================] - 18s 4ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 6.9462e-04 - val_mse: 6.9462e-04\n",
      "Epoch 100/100\n",
      "4512/4512 [==============================] - 19s 4ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 7.2661e-04 - val_mse: 7.2661e-04\n",
      "[[ 2.2067978   1.026208    1.184747   ...  1.3410532  -0.5454157\n",
      "  -0.825917  ]\n",
      " [ 2.3823833   1.3268882   1.9373375  ...  2.138962   -1.2071139\n",
      "  -2.772053  ]\n",
      " [ 0.9578668   1.8248445   2.4500315  ...  2.1457937  -0.48051307\n",
      "  -1.0478683 ]\n",
      " ...\n",
      " [ 0.14164965  0.19007596  0.10339586 ...  0.04277109 -0.0919289\n",
      "  -0.15453154]\n",
      " [ 0.15855005  0.25262266  0.14481385 ...  0.14399955 -0.10737371\n",
      "  -0.16519909]\n",
      " [ 0.17761922  0.3138656   0.12451543 ...  0.14650868 -0.07489331\n",
      "  -0.19288005]]\n"
     ]
    }
   ],
   "source": [
    "history = auto_encoder.fit(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, X_test)\n",
    ")\n",
    "auto_encoder.summary()\n",
    "auto_encoder.encoder.summary()\n",
    "auto_encoder.decoder.summary()\n",
    "\n",
    "history = auto_encoder.fit(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, X_test)\n",
    ")\n",
    "\n",
    "encoded = auto_encoder.encoder(X).numpy()\n",
    "decoded = auto_encoder.decoder(encoded).numpy()\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412457, 40)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.vectors = word_vectors.vectors[:,0:40] \n",
    "word_vectors.vector_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, vector in zip(list(word_vectors.key_to_index.values()), encoded):\n",
    "    word_vectors.vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22479425,  0.10453413,  0.1206836 , ...,  0.13660567,\n",
       "        -0.05555847, -0.08413158],\n",
       "       [ 0.12412136,  0.06913043,  0.10093462, ...,  0.1114392 ,\n",
       "        -0.06289022, -0.14442302],\n",
       "       [ 0.07095607,  0.13517934,  0.18149143, ...,  0.15895435,\n",
       "        -0.03559506, -0.07762313],\n",
       "       ...,\n",
       "       [ 0.16969655,  0.22771138,  0.12386845, ...,  0.05123984,\n",
       "        -0.110131  , -0.18512909],\n",
       "       [ 0.15769629,  0.25126234,  0.14403404, ...,  0.14322414,\n",
       "        -0.10679551, -0.16430952],\n",
       "       [ 0.18754789,  0.33141026,  0.13147567, ...,  0.15469831,\n",
       "        -0.07907975, -0.20366178]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.syn0norm = None\n",
    "word_vectors.fill_norms(force=True)\n",
    "word_vectors.get_normed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geliyoruz', 0.9236549735069275),\n",
       " ('geldiğimde', 0.9195456504821777),\n",
       " ('rastladım', 0.9137952327728271),\n",
       " ('bağladım', 0.9092761278152466),\n",
       " ('yerleşin', 0.9069922566413879),\n",
       " ('gideceğim', 0.9050990343093872),\n",
       " ('yerleş', 0.9029061794281006),\n",
       " ('yardan', 0.901644766330719),\n",
       " ('gideceksin', 0.9004161357879639),\n",
       " ('yağmış', 0.8982545733451843)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "word_vectors.most_similar(positive=[\"geliyor\",\"gitmek\"],negative=[\"gelmek\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lahana', 0.9346223473548889),\n",
       " ('pancar', 0.9302788972854614),\n",
       " ('kızartması', 0.9274839758872986),\n",
       " ('ezmesi', 0.925815761089325),\n",
       " ('ıspanak', 0.923117995262146),\n",
       " ('turşusu', 0.9229980111122131),\n",
       " ('kartol', 0.9228886961936951),\n",
       " ('erişte', 0.9220452308654785),\n",
       " ('karalahana', 0.9209051728248596),\n",
       " ('miso', 0.9204742908477783)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "word_vectors.most_similar(positive=[\"brokoli\",\"patates\"],negative=[\"meyve\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('web', 0.9463964700698853),\n",
       " ('facebook', 0.9444456696510315),\n",
       " ('blog', 0.9443336129188538),\n",
       " ('paylaşım', 0.9389534592628479),\n",
       " ('mynet', 0.9324177503585815),\n",
       " ('Web', 0.9304092526435852),\n",
       " ('ınternet', 0.9287830591201782),\n",
       " ('Wordpress', 0.9243919253349304),\n",
       " ('ağ', 0.9243459701538086),\n",
       " ('tumblr', 0.9240301251411438)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_key(\"internet\", topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ve</th>\n",
       "      <td>2.206798</td>\n",
       "      <td>1.026208</td>\n",
       "      <td>1.184747</td>\n",
       "      <td>0.593272</td>\n",
       "      <td>2.577303</td>\n",
       "      <td>2.871521</td>\n",
       "      <td>-1.364984</td>\n",
       "      <td>1.016538</td>\n",
       "      <td>0.341405</td>\n",
       "      <td>0.308782</td>\n",
       "      <td>...</td>\n",
       "      <td>1.395383</td>\n",
       "      <td>2.191643</td>\n",
       "      <td>0.809908</td>\n",
       "      <td>-0.077335</td>\n",
       "      <td>1.542955</td>\n",
       "      <td>1.110401</td>\n",
       "      <td>1.876573</td>\n",
       "      <td>1.341053</td>\n",
       "      <td>-0.545416</td>\n",
       "      <td>-0.825917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kategori</th>\n",
       "      <td>2.382383</td>\n",
       "      <td>1.326888</td>\n",
       "      <td>1.937338</td>\n",
       "      <td>0.482670</td>\n",
       "      <td>-0.213720</td>\n",
       "      <td>2.293994</td>\n",
       "      <td>-1.004302</td>\n",
       "      <td>0.668673</td>\n",
       "      <td>1.220763</td>\n",
       "      <td>-0.061286</td>\n",
       "      <td>...</td>\n",
       "      <td>3.205154</td>\n",
       "      <td>2.362246</td>\n",
       "      <td>0.227907</td>\n",
       "      <td>6.962224</td>\n",
       "      <td>0.734765</td>\n",
       "      <td>-0.481548</td>\n",
       "      <td>0.574860</td>\n",
       "      <td>2.138962</td>\n",
       "      <td>-1.207114</td>\n",
       "      <td>-2.772053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bir</th>\n",
       "      <td>0.957867</td>\n",
       "      <td>1.824844</td>\n",
       "      <td>2.450032</td>\n",
       "      <td>1.767411</td>\n",
       "      <td>3.953114</td>\n",
       "      <td>4.714775</td>\n",
       "      <td>-2.716141</td>\n",
       "      <td>3.286252</td>\n",
       "      <td>-0.096642</td>\n",
       "      <td>0.705466</td>\n",
       "      <td>...</td>\n",
       "      <td>1.009294</td>\n",
       "      <td>2.402153</td>\n",
       "      <td>2.724217</td>\n",
       "      <td>-0.164441</td>\n",
       "      <td>2.067279</td>\n",
       "      <td>0.722448</td>\n",
       "      <td>1.331011</td>\n",
       "      <td>2.145794</td>\n",
       "      <td>-0.480513</td>\n",
       "      <td>-1.047868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>2.150470</td>\n",
       "      <td>4.129030</td>\n",
       "      <td>2.423162</td>\n",
       "      <td>1.271311</td>\n",
       "      <td>4.176322</td>\n",
       "      <td>5.218581</td>\n",
       "      <td>-1.219997</td>\n",
       "      <td>1.913177</td>\n",
       "      <td>-1.945502</td>\n",
       "      <td>2.582092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169896</td>\n",
       "      <td>1.646575</td>\n",
       "      <td>1.768944</td>\n",
       "      <td>1.060161</td>\n",
       "      <td>1.850431</td>\n",
       "      <td>1.426014</td>\n",
       "      <td>1.182015</td>\n",
       "      <td>2.510493</td>\n",
       "      <td>-1.137761</td>\n",
       "      <td>-1.906018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>2.007289</td>\n",
       "      <td>3.745586</td>\n",
       "      <td>2.680676</td>\n",
       "      <td>1.159703</td>\n",
       "      <td>4.277326</td>\n",
       "      <td>5.012633</td>\n",
       "      <td>-1.324931</td>\n",
       "      <td>1.115746</td>\n",
       "      <td>-0.679559</td>\n",
       "      <td>2.133528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286980</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>1.825454</td>\n",
       "      <td>0.860004</td>\n",
       "      <td>1.629969</td>\n",
       "      <td>1.857992</td>\n",
       "      <td>0.412137</td>\n",
       "      <td>3.235987</td>\n",
       "      <td>-0.798860</td>\n",
       "      <td>-1.830661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zırhlardan</th>\n",
       "      <td>0.079502</td>\n",
       "      <td>0.298920</td>\n",
       "      <td>0.126667</td>\n",
       "      <td>0.079523</td>\n",
       "      <td>0.225341</td>\n",
       "      <td>0.169391</td>\n",
       "      <td>-0.179397</td>\n",
       "      <td>0.123934</td>\n",
       "      <td>-0.103940</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182464</td>\n",
       "      <td>0.126936</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>0.130266</td>\n",
       "      <td>0.123055</td>\n",
       "      <td>0.151548</td>\n",
       "      <td>0.139524</td>\n",
       "      <td>0.175488</td>\n",
       "      <td>-0.031088</td>\n",
       "      <td>-0.156149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanklardı</th>\n",
       "      <td>0.151478</td>\n",
       "      <td>0.252644</td>\n",
       "      <td>0.157573</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>0.277283</td>\n",
       "      <td>0.220984</td>\n",
       "      <td>-0.169026</td>\n",
       "      <td>0.109878</td>\n",
       "      <td>-0.112233</td>\n",
       "      <td>0.074280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212731</td>\n",
       "      <td>0.148030</td>\n",
       "      <td>0.189897</td>\n",
       "      <td>0.032477</td>\n",
       "      <td>0.133748</td>\n",
       "      <td>0.231958</td>\n",
       "      <td>0.145211</td>\n",
       "      <td>0.180077</td>\n",
       "      <td>-0.109420</td>\n",
       "      <td>-0.229828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bēkon</th>\n",
       "      <td>0.141650</td>\n",
       "      <td>0.190076</td>\n",
       "      <td>0.103396</td>\n",
       "      <td>0.061164</td>\n",
       "      <td>0.195010</td>\n",
       "      <td>0.249845</td>\n",
       "      <td>-0.127987</td>\n",
       "      <td>0.057267</td>\n",
       "      <td>-0.146276</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123661</td>\n",
       "      <td>0.082751</td>\n",
       "      <td>0.078318</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>0.121922</td>\n",
       "      <td>0.143550</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.042771</td>\n",
       "      <td>-0.091929</td>\n",
       "      <td>-0.154532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coosemans</th>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.252623</td>\n",
       "      <td>0.144814</td>\n",
       "      <td>0.097701</td>\n",
       "      <td>0.218991</td>\n",
       "      <td>0.098823</td>\n",
       "      <td>-0.139720</td>\n",
       "      <td>0.169183</td>\n",
       "      <td>-0.072500</td>\n",
       "      <td>0.071288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150008</td>\n",
       "      <td>0.206961</td>\n",
       "      <td>0.141978</td>\n",
       "      <td>0.181806</td>\n",
       "      <td>0.196731</td>\n",
       "      <td>0.192109</td>\n",
       "      <td>0.187174</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>-0.107374</td>\n",
       "      <td>-0.165199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halett</th>\n",
       "      <td>0.177619</td>\n",
       "      <td>0.313866</td>\n",
       "      <td>0.124515</td>\n",
       "      <td>0.199472</td>\n",
       "      <td>0.230476</td>\n",
       "      <td>0.201294</td>\n",
       "      <td>-0.134579</td>\n",
       "      <td>0.122960</td>\n",
       "      <td>-0.052601</td>\n",
       "      <td>0.063745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119043</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>0.068140</td>\n",
       "      <td>0.097612</td>\n",
       "      <td>0.166059</td>\n",
       "      <td>0.181146</td>\n",
       "      <td>0.141529</td>\n",
       "      <td>0.146509</td>\n",
       "      <td>-0.074893</td>\n",
       "      <td>-0.192880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412457 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "ve          2.206798  1.026208  1.184747  0.593272  2.577303  2.871521   \n",
       "kategori    2.382383  1.326888  1.937338  0.482670 -0.213720  2.293994   \n",
       "bir         0.957867  1.824844  2.450032  1.767411  3.953114  4.714775   \n",
       "da          2.150470  4.129030  2.423162  1.271311  4.176322  5.218581   \n",
       "de          2.007289  3.745586  2.680676  1.159703  4.277326  5.012633   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "zırhlardan  0.079502  0.298920  0.126667  0.079523  0.225341  0.169391   \n",
       "tanklardı   0.151478  0.252644  0.157573  0.140643  0.277283  0.220984   \n",
       "bēkon       0.141650  0.190076  0.103396  0.061164  0.195010  0.249845   \n",
       "coosemans   0.158550  0.252623  0.144814  0.097701  0.218991  0.098823   \n",
       "halett      0.177619  0.313866  0.124515  0.199472  0.230476  0.201294   \n",
       "\n",
       "                  6         7         8         9   ...        30        31  \\\n",
       "ve         -1.364984  1.016538  0.341405  0.308782  ...  1.395383  2.191643   \n",
       "kategori   -1.004302  0.668673  1.220763 -0.061286  ...  3.205154  2.362246   \n",
       "bir        -2.716141  3.286252 -0.096642  0.705466  ...  1.009294  2.402153   \n",
       "da         -1.219997  1.913177 -1.945502  2.582092  ... -0.169896  1.646575   \n",
       "de         -1.324931  1.115746 -0.679559  2.133528  ... -0.286980  0.417521   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "zırhlardan -0.179397  0.123934 -0.103940  0.102190  ...  0.182464  0.126936   \n",
       "tanklardı  -0.169026  0.109878 -0.112233  0.074280  ...  0.212731  0.148030   \n",
       "bēkon      -0.127987  0.057267 -0.146276  0.019158  ...  0.123661  0.082751   \n",
       "coosemans  -0.139720  0.169183 -0.072500  0.071288  ...  0.150008  0.206961   \n",
       "halett     -0.134579  0.122960 -0.052601  0.063745  ...  0.119043  0.106768   \n",
       "\n",
       "                  32        33        34        35        36        37  \\\n",
       "ve          0.809908 -0.077335  1.542955  1.110401  1.876573  1.341053   \n",
       "kategori    0.227907  6.962224  0.734765 -0.481548  0.574860  2.138962   \n",
       "bir         2.724217 -0.164441  2.067279  0.722448  1.331011  2.145794   \n",
       "da          1.768944  1.060161  1.850431  1.426014  1.182015  2.510493   \n",
       "de          1.825454  0.860004  1.629969  1.857992  0.412137  3.235987   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "zırhlardan  0.218056  0.130266  0.123055  0.151548  0.139524  0.175488   \n",
       "tanklardı   0.189897  0.032477  0.133748  0.231958  0.145211  0.180077   \n",
       "bēkon       0.078318 -0.001527  0.121922  0.143550  0.058766  0.042771   \n",
       "coosemans   0.141978  0.181806  0.196731  0.192109  0.187174  0.144000   \n",
       "halett      0.068140  0.097612  0.166059  0.181146  0.141529  0.146509   \n",
       "\n",
       "                  38        39  \n",
       "ve         -0.545416 -0.825917  \n",
       "kategori   -1.207114 -2.772053  \n",
       "bir        -0.480513 -1.047868  \n",
       "da         -1.137761 -1.906018  \n",
       "de         -0.798860 -1.830661  \n",
       "...              ...       ...  \n",
       "zırhlardan -0.031088 -0.156149  \n",
       "tanklardı  -0.109420 -0.229828  \n",
       "bēkon      -0.091929 -0.154532  \n",
       "coosemans  -0.107374 -0.165199  \n",
       "halett     -0.074893 -0.192880  \n",
       "\n",
       "[412457 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_dict_new = {}\n",
    "for word in word_vectors.index_to_key: \n",
    "    vector_dict_new[word] = word_vectors.get_vector(word)\n",
    "\n",
    "df = pd.DataFrame(vector_dict_new).T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.save_word2vec_format(\"trmodel_truncated\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('trmodel_truncated', binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
